{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN WHEEL DEFECT DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following dataset has the values captured by the sensors placed on railway tracks.\n",
    "#Sensors detect the force applied on them.\n",
    "#Variations in the values of force occur due to three different types of defects.\n",
    "#The dataset has a defect attribute which is the class attribute.\n",
    "#The dataset has 150 rows and 5 columns.\n",
    "#Four attributes are related to the values captured by the sensors and the left over attribute is the class.\n",
    "#The class attribute has 3 different classes.\n",
    "#Classes of the defect are:\n",
    "#Flatspot\n",
    "#Shelling\n",
    "#Non-roundness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating class attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sensor_1  sensor_2  sensor_3  sensor_4     defect\n",
      "0       5.1       3.5       1.4       0.2  Flat spot\n",
      "1       4.9       3.0       1.4       0.2  Flat spot\n",
      "2       4.7       3.2       1.3       0.2  Flat spot\n",
      "3       4.6       3.1       1.5       0.2  Flat spot\n",
      "4       5.0       3.6       1.4       0.2  Flat spot\n"
     ]
    }
   ],
   "source": [
    "dataset=pd.read_csv('wheels.csv')\n",
    "print(dataset.head())\n",
    "X=dataset[['sensor_1','sensor_2','sensor_3','sensor_4']]\n",
    "y=dataset[['defect']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the class attribute values to appropriate arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.7465 - acc: 0.6762\n",
      "Epoch 2/250\n",
      "105/105 [==============================] - 0s 744us/step - loss: 0.6711 - acc: 0.6762\n",
      "Epoch 3/250\n",
      "105/105 [==============================] - 0s 658us/step - loss: 0.6266 - acc: 0.6762\n",
      "Epoch 4/250\n",
      "105/105 [==============================] - 0s 806us/step - loss: 0.5861 - acc: 0.6762\n",
      "Epoch 5/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.5555 - acc: 0.6571\n",
      "Epoch 6/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.5324 - acc: 0.7810\n",
      "Epoch 7/250\n",
      "105/105 [==============================] - 0s 389us/step - loss: 0.5113 - acc: 0.8000\n",
      "Epoch 8/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.4947 - acc: 0.8000\n",
      "Epoch 9/250\n",
      "105/105 [==============================] - 0s 462us/step - loss: 0.4825 - acc: 0.8095\n",
      "Epoch 10/250\n",
      "105/105 [==============================] - 0s 479us/step - loss: 0.4691 - acc: 0.8190\n",
      "Epoch 11/250\n",
      "105/105 [==============================] - 0s 366us/step - loss: 0.4606 - acc: 0.8476\n",
      "Epoch 12/250\n",
      "105/105 [==============================] - 0s 421us/step - loss: 0.4508 - acc: 0.8286\n",
      "Epoch 13/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.4458 - acc: 0.8000\n",
      "Epoch 14/250\n",
      "105/105 [==============================] - 0s 394us/step - loss: 0.4387 - acc: 0.8190\n",
      "Epoch 15/250\n",
      "105/105 [==============================] - 0s 318us/step - loss: 0.4301 - acc: 0.8381\n",
      "Epoch 16/250\n",
      "105/105 [==============================] - 0s 369us/step - loss: 0.4211 - acc: 0.8381\n",
      "Epoch 17/250\n",
      "105/105 [==============================] - 0s 346us/step - loss: 0.4163 - acc: 0.8571\n",
      "Epoch 18/250\n",
      "105/105 [==============================] - 0s 471us/step - loss: 0.4082 - acc: 0.8952\n",
      "Epoch 19/250\n",
      "105/105 [==============================] - 0s 755us/step - loss: 0.4061 - acc: 0.8667\n",
      "Epoch 20/250\n",
      "105/105 [==============================] - 0s 684us/step - loss: 0.4009 - acc: 0.8190\n",
      "Epoch 21/250\n",
      "105/105 [==============================] - 0s 677us/step - loss: 0.3919 - acc: 0.8667\n",
      "Epoch 22/250\n",
      "105/105 [==============================] - 0s 691us/step - loss: 0.3910 - acc: 0.8381\n",
      "Epoch 23/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.3836 - acc: 0.8952\n",
      "Epoch 24/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.3769 - acc: 0.8762\n",
      "Epoch 25/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.3711 - acc: 0.8667\n",
      "Epoch 26/250\n",
      "105/105 [==============================] - 0s 638us/step - loss: 0.3702 - acc: 0.8762\n",
      "Epoch 27/250\n",
      "105/105 [==============================] - 0s 808us/step - loss: 0.3695 - acc: 0.8857\n",
      "Epoch 28/250\n",
      "105/105 [==============================] - 0s 686us/step - loss: 0.3513 - acc: 0.8857\n",
      "Epoch 29/250\n",
      "105/105 [==============================] - 0s 714us/step - loss: 0.3514 - acc: 0.9048\n",
      "Epoch 30/250\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3477 - acc: 0.910 - 0s 1ms/step - loss: 0.3478 - acc: 0.9048\n",
      "Epoch 31/250\n",
      "105/105 [==============================] - 0s 565us/step - loss: 0.3422 - acc: 0.9143\n",
      "Epoch 32/250\n",
      "105/105 [==============================] - 0s 542us/step - loss: 0.3365 - acc: 0.8762\n",
      "Epoch 33/250\n",
      "105/105 [==============================] - 0s 657us/step - loss: 0.3289 - acc: 0.8952\n",
      "Epoch 34/250\n",
      "105/105 [==============================] - 0s 657us/step - loss: 0.3204 - acc: 0.9048\n",
      "Epoch 35/250\n",
      "105/105 [==============================] - 0s 744us/step - loss: 0.3167 - acc: 0.9238\n",
      "Epoch 36/250\n",
      "105/105 [==============================] - 0s 542us/step - loss: 0.3225 - acc: 0.9048\n",
      "Epoch 37/250\n",
      "105/105 [==============================] - 0s 624us/step - loss: 0.3080 - acc: 0.9143\n",
      "Epoch 38/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.2967 - acc: 0.9048\n",
      "Epoch 39/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.3050 - acc: 0.9048\n",
      "Epoch 40/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.2929 - acc: 0.9333\n",
      "Epoch 41/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.2901 - acc: 0.9143\n",
      "Epoch 42/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.2847 - acc: 0.9429\n",
      "Epoch 43/250\n",
      "105/105 [==============================] - 0s 657us/step - loss: 0.2793 - acc: 0.9238\n",
      "Epoch 44/250\n",
      "105/105 [==============================] - 0s 803us/step - loss: 0.2768 - acc: 0.9048\n",
      "Epoch 45/250\n",
      "105/105 [==============================] - 0s 807us/step - loss: 0.2705 - acc: 0.9333\n",
      "Epoch 46/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.2693 - acc: 0.9238\n",
      "Epoch 47/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.2548 - acc: 0.9238\n",
      "Epoch 48/250\n",
      "105/105 [==============================] - 0s 657us/step - loss: 0.2644 - acc: 0.9048\n",
      "Epoch 49/250\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2609 - acc: 0.9238\n",
      "Epoch 50/250\n",
      "105/105 [==============================] - 0s 654us/step - loss: 0.2559 - acc: 0.9238\n",
      "Epoch 51/250\n",
      "105/105 [==============================] - 0s 596us/step - loss: 0.2468 - acc: 0.9429\n",
      "Epoch 52/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.2450 - acc: 0.9238\n",
      "Epoch 53/250\n",
      "105/105 [==============================] - 0s 953us/step - loss: 0.2391 - acc: 0.9333\n",
      "Epoch 54/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.2415 - acc: 0.9143\n",
      "Epoch 55/250\n",
      "105/105 [==============================] - 0s 654us/step - loss: 0.2374 - acc: 0.9429\n",
      "Epoch 56/250\n",
      "105/105 [==============================] - 0s 596us/step - loss: 0.2304 - acc: 0.9238\n",
      "Epoch 57/250\n",
      "105/105 [==============================] - 0s 358us/step - loss: 0.2295 - acc: 0.9429\n",
      "Epoch 58/250\n",
      "105/105 [==============================] - 0s 744us/step - loss: 0.2251 - acc: 0.9238\n",
      "Epoch 59/250\n",
      "105/105 [==============================] - 0s 356us/step - loss: 0.2272 - acc: 0.9524\n",
      "Epoch 60/250\n",
      "105/105 [==============================] - 0s 636us/step - loss: 0.2260 - acc: 0.9143\n",
      "Epoch 61/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.2079 - acc: 0.9619\n",
      "Epoch 62/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.2198 - acc: 0.9238\n",
      "Epoch 63/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.2056 - acc: 0.9143\n",
      "Epoch 64/250\n",
      "105/105 [==============================] - 0s 653us/step - loss: 0.2076 - acc: 0.9524\n",
      "Epoch 65/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.2079 - acc: 0.9429\n",
      "Epoch 66/250\n",
      "105/105 [==============================] - 0s 504us/step - loss: 0.2042 - acc: 0.9238\n",
      "Epoch 67/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.2081 - acc: 0.9333\n",
      "Epoch 68/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1980 - acc: 0.9429\n",
      "Epoch 69/250\n",
      "105/105 [==============================] - 0s 505us/step - loss: 0.2008 - acc: 0.9333\n",
      "Epoch 70/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1923 - acc: 0.9143\n",
      "Epoch 71/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1867 - acc: 0.9714\n",
      "Epoch 72/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1891 - acc: 0.9619\n",
      "Epoch 73/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1870 - acc: 0.9524\n",
      "Epoch 74/250\n",
      "105/105 [==============================] - 0s 523us/step - loss: 0.1906 - acc: 0.9333\n",
      "Epoch 75/250\n",
      "105/105 [==============================] - 0s 431us/step - loss: 0.1867 - acc: 0.9429\n",
      "Epoch 76/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1860 - acc: 0.9524\n",
      "Epoch 77/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1827 - acc: 0.9524\n",
      "Epoch 78/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1755 - acc: 0.9524\n",
      "Epoch 79/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1777 - acc: 0.9333\n",
      "Epoch 80/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1861 - acc: 0.9238\n",
      "Epoch 81/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1777 - acc: 0.9524\n",
      "Epoch 82/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1813 - acc: 0.9333\n",
      "Epoch 83/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 509us/step - loss: 0.1734 - acc: 0.9524\n",
      "Epoch 84/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1770 - acc: 0.9524\n",
      "Epoch 85/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1854 - acc: 0.9524\n",
      "Epoch 86/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1669 - acc: 0.9333\n",
      "Epoch 87/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1742 - acc: 0.9429\n",
      "Epoch 88/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1653 - acc: 0.9619\n",
      "Epoch 89/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1633 - acc: 0.9619\n",
      "Epoch 90/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1590 - acc: 0.9429\n",
      "Epoch 91/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1646 - acc: 0.9619\n",
      "Epoch 92/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1615 - acc: 0.9524\n",
      "Epoch 93/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1365 - acc: 0.9429\n",
      "Epoch 94/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1581 - acc: 0.9429\n",
      "Epoch 95/250\n",
      "105/105 [==============================] - 0s 654us/step - loss: 0.1448 - acc: 0.9619\n",
      "Epoch 96/250\n",
      "105/105 [==============================] - 0s 955us/step - loss: 0.1650 - acc: 0.9619\n",
      "Epoch 97/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1638 - acc: 0.9429\n",
      "Epoch 98/250\n",
      "105/105 [==============================] - 0s 658us/step - loss: 0.1477 - acc: 0.9429\n",
      "Epoch 99/250\n",
      "105/105 [==============================] - 0s 508us/step - loss: 0.1504 - acc: 0.9524\n",
      "Epoch 100/250\n",
      "105/105 [==============================] - 0s 534us/step - loss: 0.1504 - acc: 0.9524\n",
      "Epoch 101/250\n",
      "105/105 [==============================] - 0s 417us/step - loss: 0.1542 - acc: 0.9524\n",
      "Epoch 102/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1523 - acc: 0.9619\n",
      "Epoch 103/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1468 - acc: 0.9429\n",
      "Epoch 104/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1529 - acc: 0.9524\n",
      "Epoch 105/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1460 - acc: 0.9524\n",
      "Epoch 106/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1569 - acc: 0.9333\n",
      "Epoch 107/250\n",
      "105/105 [==============================] - 0s 736us/step - loss: 0.1529 - acc: 0.9429\n",
      "Epoch 108/250\n",
      "105/105 [==============================] - 0s 524us/step - loss: 0.1521 - acc: 0.9333\n",
      "Epoch 109/250\n",
      "105/105 [==============================] - 0s 524us/step - loss: 0.1472 - acc: 0.9333\n",
      "Epoch 110/250\n",
      "105/105 [==============================] - 0s 571us/step - loss: 0.1395 - acc: 0.9619\n",
      "Epoch 111/250\n",
      "105/105 [==============================] - 0s 486us/step - loss: 0.1436 - acc: 0.9524\n",
      "Epoch 112/250\n",
      "105/105 [==============================] - 0s 676us/step - loss: 0.1566 - acc: 0.9238 0s - loss: 0.1794 - acc: 0.905\n",
      "Epoch 113/250\n",
      "105/105 [==============================] - 0s 403us/step - loss: 0.1371 - acc: 0.9619\n",
      "Epoch 114/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1477 - acc: 0.9524\n",
      "Epoch 115/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1447 - acc: 0.9619\n",
      "Epoch 116/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1554 - acc: 0.9429\n",
      "Epoch 117/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1407 - acc: 0.9524\n",
      "Epoch 118/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1407 - acc: 0.9429\n",
      "Epoch 119/250\n",
      "105/105 [==============================] - 0s 658us/step - loss: 0.1395 - acc: 0.9429\n",
      "Epoch 120/250\n",
      "105/105 [==============================] - 0s 587us/step - loss: 0.1281 - acc: 0.9524\n",
      "Epoch 121/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1384 - acc: 0.9714\n",
      "Epoch 122/250\n",
      "105/105 [==============================] - 0s 426us/step - loss: 0.1407 - acc: 0.9524\n",
      "Epoch 123/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1286 - acc: 0.9619\n",
      "Epoch 124/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1310 - acc: 0.9524\n",
      "Epoch 125/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1545 - acc: 0.9429\n",
      "Epoch 126/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1324 - acc: 0.9524\n",
      "Epoch 127/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1280 - acc: 0.9524\n",
      "Epoch 128/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1402 - acc: 0.9333\n",
      "Epoch 129/250\n",
      "105/105 [==============================] - 0s 597us/step - loss: 0.1304 - acc: 0.9619\n",
      "Epoch 130/250\n",
      "105/105 [==============================] - 0s 326us/step - loss: 0.1414 - acc: 0.9524\n",
      "Epoch 131/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1305 - acc: 0.9619\n",
      "Epoch 132/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1279 - acc: 0.9524\n",
      "Epoch 133/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1181 - acc: 0.9524\n",
      "Epoch 134/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1307 - acc: 0.9333\n",
      "Epoch 135/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1246 - acc: 0.9619\n",
      "Epoch 136/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1172 - acc: 0.9714\n",
      "Epoch 137/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1407 - acc: 0.9524\n",
      "Epoch 138/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1226 - acc: 0.9619\n",
      "Epoch 139/250\n",
      "105/105 [==============================] - 0s 658us/step - loss: 0.1321 - acc: 0.9429\n",
      "Epoch 140/250\n",
      "105/105 [==============================] - 0s 657us/step - loss: 0.1308 - acc: 0.9714\n",
      "Epoch 141/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1343 - acc: 0.9619\n",
      "Epoch 142/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1353 - acc: 0.9524\n",
      "Epoch 143/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1266 - acc: 0.9524\n",
      "Epoch 144/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1224 - acc: 0.9429\n",
      "Epoch 145/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1245 - acc: 0.9429\n",
      "Epoch 146/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1205 - acc: 0.9714\n",
      "Epoch 147/250\n",
      "105/105 [==============================] - 0s 361us/step - loss: 0.1206 - acc: 0.9714\n",
      "Epoch 148/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1251 - acc: 0.9619\n",
      "Epoch 149/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1354 - acc: 0.9333\n",
      "Epoch 150/250\n",
      "105/105 [==============================] - 0s 479us/step - loss: 0.1184 - acc: 0.9714\n",
      "Epoch 151/250\n",
      "105/105 [==============================] - 0s 317us/step - loss: 0.1251 - acc: 0.9524\n",
      "Epoch 152/250\n",
      "105/105 [==============================] - 0s 596us/step - loss: 0.1271 - acc: 0.9714\n",
      "Epoch 153/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1285 - acc: 0.9524\n",
      "Epoch 154/250\n",
      "105/105 [==============================] - 0s 320us/step - loss: 0.1225 - acc: 0.9524\n",
      "Epoch 155/250\n",
      "105/105 [==============================] - 0s 336us/step - loss: 0.1334 - acc: 0.9429\n",
      "Epoch 156/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1436 - acc: 0.9333\n",
      "Epoch 157/250\n",
      "105/105 [==============================] - 0s 508us/step - loss: 0.1209 - acc: 0.9429\n",
      "Epoch 158/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1204 - acc: 0.9619\n",
      "Epoch 159/250\n",
      "105/105 [==============================] - 0s 508us/step - loss: 0.1402 - acc: 0.9238\n",
      "Epoch 160/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1169 - acc: 0.9524\n",
      "Epoch 161/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1233 - acc: 0.9524\n",
      "Epoch 162/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1210 - acc: 0.9714\n",
      "Epoch 163/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1238 - acc: 0.9429\n",
      "Epoch 164/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 447us/step - loss: 0.1158 - acc: 0.9714\n",
      "Epoch 165/250\n",
      "105/105 [==============================] - 0s 508us/step - loss: 0.1127 - acc: 0.9619\n",
      "Epoch 166/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1214 - acc: 0.9524\n",
      "Epoch 167/250\n",
      "105/105 [==============================] - 0s 508us/step - loss: 0.1181 - acc: 0.9524\n",
      "Epoch 168/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1272 - acc: 0.9524\n",
      "Epoch 169/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1279 - acc: 0.9333\n",
      "Epoch 170/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1367 - acc: 0.9333\n",
      "Epoch 171/250\n",
      "105/105 [==============================] - 0s 458us/step - loss: 0.1191 - acc: 0.9714\n",
      "Epoch 172/250\n",
      "105/105 [==============================] - 0s 495us/step - loss: 0.1134 - acc: 0.9619\n",
      "Epoch 173/250\n",
      "105/105 [==============================] - 0s 657us/step - loss: 0.1130 - acc: 0.9619\n",
      "Epoch 174/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1385 - acc: 0.9714\n",
      "Epoch 175/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1195 - acc: 0.9333\n",
      "Epoch 176/250\n",
      "105/105 [==============================] - 0s 211us/step - loss: 0.0953 - acc: 0.9714\n",
      "Epoch 177/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1196 - acc: 0.9429\n",
      "Epoch 178/250\n",
      "105/105 [==============================] - 0s 211us/step - loss: 0.1383 - acc: 0.9238\n",
      "Epoch 179/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1140 - acc: 0.9714\n",
      "Epoch 180/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1354 - acc: 0.9429\n",
      "Epoch 181/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1103 - acc: 0.9714\n",
      "Epoch 182/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.0995 - acc: 0.9619\n",
      "Epoch 183/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1308 - acc: 0.9238\n",
      "Epoch 184/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1173 - acc: 0.9524\n",
      "Epoch 185/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1056 - acc: 0.9619\n",
      "Epoch 186/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1142 - acc: 0.9619\n",
      "Epoch 187/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1100 - acc: 0.9619\n",
      "Epoch 188/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1037 - acc: 0.9810\n",
      "Epoch 189/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1113 - acc: 0.9619\n",
      "Epoch 190/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.1117 - acc: 0.9524\n",
      "Epoch 191/250\n",
      "105/105 [==============================] - 0s 356us/step - loss: 0.1094 - acc: 0.9524\n",
      "Epoch 192/250\n",
      "105/105 [==============================] - 0s 471us/step - loss: 0.1132 - acc: 0.9619\n",
      "Epoch 193/250\n",
      "105/105 [==============================] - 0s 465us/step - loss: 0.1386 - acc: 0.9333\n",
      "Epoch 194/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1194 - acc: 0.9429\n",
      "Epoch 195/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1174 - acc: 0.9619\n",
      "Epoch 196/250\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1152 - acc: 0.9524\n",
      "Epoch 197/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1109 - acc: 0.9429 0s - loss: 0.1143 - acc: 0.940\n",
      "Epoch 198/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1183 - acc: 0.9714\n",
      "Epoch 199/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1145 - acc: 0.9714\n",
      "Epoch 200/250\n",
      "105/105 [==============================] - 0s 745us/step - loss: 0.1173 - acc: 0.9524\n",
      "Epoch 201/250\n",
      "105/105 [==============================] - 0s 508us/step - loss: 0.1169 - acc: 0.9524\n",
      "Epoch 202/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1184 - acc: 0.9524\n",
      "Epoch 203/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1112 - acc: 0.9619\n",
      "Epoch 204/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1021 - acc: 0.9619\n",
      "Epoch 205/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.0995 - acc: 0.9619\n",
      "Epoch 206/250\n",
      "105/105 [==============================] - 0s 659us/step - loss: 0.0994 - acc: 0.9714\n",
      "Epoch 207/250\n",
      "105/105 [==============================] - 0s 429us/step - loss: 0.1057 - acc: 0.9619\n",
      "Epoch 208/250\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.1034 - acc: 0.9524\n",
      "Epoch 209/250\n",
      "105/105 [==============================] - 0s 486us/step - loss: 0.1041 - acc: 0.9524\n",
      "Epoch 210/250\n",
      "105/105 [==============================] - 0s 457us/step - loss: 0.1188 - acc: 0.9619\n",
      "Epoch 211/250\n",
      "105/105 [==============================] - 0s 495us/step - loss: 0.1278 - acc: 0.9429\n",
      "Epoch 212/250\n",
      "105/105 [==============================] - 0s 513us/step - loss: 0.1069 - acc: 0.9619\n",
      "Epoch 213/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1045 - acc: 0.9714\n",
      "Epoch 214/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1064 - acc: 0.9714\n",
      "Epoch 215/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1051 - acc: 0.9714\n",
      "Epoch 216/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1031 - acc: 0.9619\n",
      "Epoch 217/250\n",
      "105/105 [==============================] - 0s 470us/step - loss: 0.1101 - acc: 0.9524\n",
      "Epoch 218/250\n",
      "105/105 [==============================] - 0s 485us/step - loss: 0.1256 - acc: 0.9429\n",
      "Epoch 219/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1171 - acc: 0.9524\n",
      "Epoch 220/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1188 - acc: 0.9524\n",
      "Epoch 221/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1108 - acc: 0.9524\n",
      "Epoch 222/250\n",
      "105/105 [==============================] - 0s 351us/step - loss: 0.1018 - acc: 0.9619\n",
      "Epoch 223/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1180 - acc: 0.9714\n",
      "Epoch 224/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1031 - acc: 0.9810\n",
      "Epoch 225/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1118 - acc: 0.9524\n",
      "Epoch 226/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1017 - acc: 0.9524\n",
      "Epoch 227/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1056 - acc: 0.9429\n",
      "Epoch 228/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.0993 - acc: 0.9619\n",
      "Epoch 229/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1160 - acc: 0.9619\n",
      "Epoch 230/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.0957 - acc: 0.9619\n",
      "Epoch 231/250\n",
      "105/105 [==============================] - 0s 503us/step - loss: 0.1365 - acc: 0.9333\n",
      "Epoch 232/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1104 - acc: 0.9619\n",
      "Epoch 233/250\n",
      "105/105 [==============================] - 0s 658us/step - loss: 0.1276 - acc: 0.9524\n",
      "Epoch 234/250\n",
      "105/105 [==============================] - 0s 657us/step - loss: 0.1037 - acc: 0.9714\n",
      "Epoch 235/250\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.1129 - acc: 0.9619\n",
      "Epoch 236/250\n",
      "105/105 [==============================] - 0s 658us/step - loss: 0.1174 - acc: 0.9429\n",
      "Epoch 237/250\n",
      "105/105 [==============================] - 0s 657us/step - loss: 0.1234 - acc: 0.9524\n",
      "Epoch 238/250\n",
      "105/105 [==============================] - 0s 596us/step - loss: 0.1048 - acc: 0.9714\n",
      "Epoch 239/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1148 - acc: 0.9524\n",
      "Epoch 240/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1260 - acc: 0.9333\n",
      "Epoch 241/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1192 - acc: 0.9333\n",
      "Epoch 242/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.1122 - acc: 0.9619\n",
      "Epoch 243/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.1043 - acc: 0.9619\n",
      "Epoch 244/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.0997 - acc: 0.9524\n",
      "Epoch 245/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 446us/step - loss: 0.1039 - acc: 0.9714\n",
      "Epoch 246/250\n",
      "105/105 [==============================] - 0s 509us/step - loss: 0.1017 - acc: 0.9714\n",
      "Epoch 247/250\n",
      "105/105 [==============================] - 0s 468us/step - loss: 0.1007 - acc: 0.9524\n",
      "Epoch 248/250\n",
      "105/105 [==============================] - 0s 634us/step - loss: 0.1139 - acc: 0.9619\n",
      "Epoch 249/250\n",
      "105/105 [==============================] - 0s 508us/step - loss: 0.1064 - acc: 0.9619\n",
      "Epoch 250/250\n",
      "105/105 [==============================] - 0s 468us/step - loss: 0.1215 - acc: 0.9524\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "sgd = optimizers.SGD(lr=0.3, momentum=0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=250, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting results for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "pred = list()\n",
    "for i in range(len(y_pred)):\n",
    "    pred.append(np.argmax(y_pred[i]))\n",
    "test = list()\n",
    "for i in range(len(y_test)):\n",
    "    test.append(np.argmax(y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate accuracy and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 97.77777777777777\n",
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/250\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0999 - acc: 0.9714 - val_loss: 0.0480 - val_acc: 1.0000\n",
      "Epoch 2/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0914 - acc: 0.9714 - val_loss: 0.0456 - val_acc: 1.0000\n",
      "Epoch 3/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0981 - acc: 0.9714 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 4/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0962 - acc: 0.9714 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 5/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0916 - acc: 0.9714 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 6/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0928 - acc: 0.9714 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "Epoch 7/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0918 - acc: 0.9714 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "Epoch 8/250\n",
      "105/105 [==============================] - 0s 232us/step - loss: 0.0924 - acc: 0.9714 - val_loss: 0.0463 - val_acc: 1.0000\n",
      "Epoch 9/250\n",
      "105/105 [==============================] - 0s 312us/step - loss: 0.0910 - acc: 0.9714 - val_loss: 0.0456 - val_acc: 1.0000\n",
      "Epoch 10/250\n",
      "105/105 [==============================] - 0s 174us/step - loss: 0.0902 - acc: 0.9714 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 11/250\n",
      "105/105 [==============================] - 0s 270us/step - loss: 0.0912 - acc: 0.9714 - val_loss: 0.0454 - val_acc: 1.0000\n",
      "Epoch 12/250\n",
      "105/105 [==============================] - 0s 197us/step - loss: 0.0915 - acc: 0.9714 - val_loss: 0.0466 - val_acc: 1.0000\n",
      "Epoch 13/250\n",
      "105/105 [==============================] - 0s 97us/step - loss: 0.0907 - acc: 0.9714 - val_loss: 0.0457 - val_acc: 1.0000\n",
      "Epoch 14/250\n",
      "105/105 [==============================] - 0s 176us/step - loss: 0.0928 - acc: 0.9714 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 15/250\n",
      "105/105 [==============================] - 0s 193us/step - loss: 0.0907 - acc: 0.9714 - val_loss: 0.0449 - val_acc: 1.0000\n",
      "Epoch 16/250\n",
      "105/105 [==============================] - 0s 194us/step - loss: 0.0929 - acc: 0.9714 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "Epoch 17/250\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0909 - acc: 0.9810 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 18/250\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.0930 - acc: 0.9714 - val_loss: 0.0455 - val_acc: 1.0000\n",
      "Epoch 19/250\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.0231 - acc: 1.000 - 0s 174us/step - loss: 0.0904 - acc: 0.9714 - val_loss: 0.0458 - val_acc: 1.0000\n",
      "Epoch 20/250\n",
      "105/105 [==============================] - 0s 173us/step - loss: 0.0911 - acc: 0.9714 - val_loss: 0.0450 - val_acc: 1.0000\n",
      "Epoch 21/250\n",
      "105/105 [==============================] - 0s 97us/step - loss: 0.0907 - acc: 0.9714 - val_loss: 0.0458 - val_acc: 1.0000\n",
      "Epoch 22/250\n",
      "105/105 [==============================] - 0s 116us/step - loss: 0.0935 - acc: 0.9714 - val_loss: 0.0468 - val_acc: 1.0000\n",
      "Epoch 23/250\n",
      "105/105 [==============================] - 0s 173us/step - loss: 0.0912 - acc: 0.9714 - val_loss: 0.0453 - val_acc: 1.0000\n",
      "Epoch 24/250\n",
      "105/105 [==============================] - 0s 174us/step - loss: 0.0906 - acc: 0.9714 - val_loss: 0.0457 - val_acc: 1.0000\n",
      "Epoch 25/250\n",
      "105/105 [==============================] - 0s 194us/step - loss: 0.0937 - acc: 0.9714 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 26/250\n",
      "105/105 [==============================] - 0s 155us/step - loss: 0.0923 - acc: 0.9714 - val_loss: 0.0450 - val_acc: 1.0000\n",
      "Epoch 27/250\n",
      "105/105 [==============================] - 0s 193us/step - loss: 0.0935 - acc: 0.9714 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 28/250\n",
      "105/105 [==============================] - 0s 96us/step - loss: 0.0946 - acc: 0.9810 - val_loss: 0.0447 - val_acc: 1.0000\n",
      "Epoch 29/250\n",
      "105/105 [==============================] - 0s 96us/step - loss: 0.0983 - acc: 0.9714 - val_loss: 0.0446 - val_acc: 1.0000\n",
      "Epoch 30/250\n",
      "105/105 [==============================] - 0s 116us/step - loss: 0.0923 - acc: 0.9714 - val_loss: 0.0453 - val_acc: 1.0000\n",
      "Epoch 31/250\n",
      "105/105 [==============================] - 0s 193us/step - loss: 0.0931 - acc: 0.9714 - val_loss: 0.0464 - val_acc: 1.0000\n",
      "Epoch 32/250\n",
      "105/105 [==============================] - 0s 77us/step - loss: 0.0908 - acc: 0.9714 - val_loss: 0.0449 - val_acc: 1.0000\n",
      "Epoch 33/250\n",
      "105/105 [==============================] - 0s 194us/step - loss: 0.0916 - acc: 0.9714 - val_loss: 0.0449 - val_acc: 1.0000\n",
      "Epoch 34/250\n",
      "105/105 [==============================] - 0s 193us/step - loss: 0.0904 - acc: 0.9714 - val_loss: 0.0446 - val_acc: 1.0000\n",
      "Epoch 35/250\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.1311 - acc: 0.950 - 0s 97us/step - loss: 0.0907 - acc: 0.9714 - val_loss: 0.0456 - val_acc: 1.0000\n",
      "Epoch 36/250\n",
      "105/105 [==============================] - 0s 197us/step - loss: 0.0902 - acc: 0.9714 - val_loss: 0.0447 - val_acc: 1.0000\n",
      "Epoch 37/250\n",
      "105/105 [==============================] - 0s 194us/step - loss: 0.0932 - acc: 0.9714 - val_loss: 0.0445 - val_acc: 1.0000\n",
      "Epoch 38/250\n",
      "105/105 [==============================] - 0s 173us/step - loss: 0.0904 - acc: 0.9714 - val_loss: 0.0448 - val_acc: 1.0000\n",
      "Epoch 39/250\n",
      "105/105 [==============================] - 0s 77us/step - loss: 0.0916 - acc: 0.9714 - val_loss: 0.0461 - val_acc: 1.0000\n",
      "Epoch 40/250\n",
      "105/105 [==============================] - 0s 212us/step - loss: 0.0902 - acc: 0.9714 - val_loss: 0.0448 - val_acc: 1.0000\n",
      "Epoch 41/250\n",
      "105/105 [==============================] - 0s 96us/step - loss: 0.0918 - acc: 0.9714 - val_loss: 0.0460 - val_acc: 1.0000\n",
      "Epoch 42/250\n",
      "105/105 [==============================] - 0s 173us/step - loss: 0.0914 - acc: 0.9714 - val_loss: 0.0455 - val_acc: 1.0000\n",
      "Epoch 43/250\n",
      "105/105 [==============================] - 0s 165us/step - loss: 0.0907 - acc: 0.9714 - val_loss: 0.0447 - val_acc: 1.0000\n",
      "Epoch 44/250\n",
      "105/105 [==============================] - 0s 57us/step - loss: 0.0913 - acc: 0.9714 - val_loss: 0.0444 - val_acc: 1.0000\n",
      "Epoch 45/250\n",
      "105/105 [==============================] - 0s 96us/step - loss: 0.0899 - acc: 0.9714 - val_loss: 0.0444 - val_acc: 1.0000\n",
      "Epoch 46/250\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0952 - acc: 0.9714 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 47/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0931 - acc: 0.9714 - val_loss: 0.0467 - val_acc: 1.0000\n",
      "Epoch 48/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0909 - acc: 0.9714 - val_loss: 0.0444 - val_acc: 1.0000\n",
      "Epoch 49/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0905 - acc: 0.9714 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 50/250\n",
      "105/105 [==============================] - 0s 148us/step - loss: 0.0904 - acc: 0.9714 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 51/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.0926 - acc: 0.9714 - val_loss: 0.0448 - val_acc: 1.0000\n",
      "Epoch 52/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0918 - acc: 0.9905 - val_loss: 0.0442 - val_acc: 1.0000\n",
      "Epoch 53/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0927 - acc: 0.9714 - val_loss: 0.0444 - val_acc: 1.0000\n",
      "Epoch 54/250\n",
      "105/105 [==============================] - 0s 148us/step - loss: 0.0914 - acc: 0.9810 - val_loss: 0.0448 - val_acc: 1.0000\n",
      "Epoch 55/250\n",
      "105/105 [==============================] - 0s 211us/step - loss: 0.0913 - acc: 0.9714 - val_loss: 0.0466 - val_acc: 1.0000\n",
      "Epoch 56/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0906 - acc: 0.9714 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 57/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0894 - acc: 0.9714 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 58/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0897 - acc: 0.9714 - val_loss: 0.0450 - val_acc: 1.0000\n",
      "Epoch 59/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0907 - acc: 0.9714 - val_loss: 0.0448 - val_acc: 1.0000\n",
      "Epoch 60/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 148us/step - loss: 0.0910 - acc: 0.9714 - val_loss: 0.0456 - val_acc: 1.0000\n",
      "Epoch 61/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0910 - acc: 0.9714 - val_loss: 0.0441 - val_acc: 1.0000\n",
      "Epoch 62/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0909 - acc: 0.9714 - val_loss: 0.0442 - val_acc: 1.0000\n",
      "Epoch 63/250\n",
      "105/105 [==============================] - 0s 150us/step - loss: 0.0932 - acc: 0.9714 - val_loss: 0.0444 - val_acc: 1.0000\n",
      "Epoch 64/250\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.0911 - acc: 0.9714 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "Epoch 65/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0899 - acc: 0.9714 - val_loss: 0.0441 - val_acc: 1.0000\n",
      "Epoch 66/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0947 - acc: 0.9619 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "Epoch 67/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0902 - acc: 0.9714 - val_loss: 0.0445 - val_acc: 1.0000\n",
      "Epoch 68/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0905 - acc: 0.9714 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 69/250\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0898 - acc: 0.9714 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 70/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0913 - acc: 0.9714 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "Epoch 71/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0898 - acc: 0.9714 - val_loss: 0.0439 - val_acc: 1.0000\n",
      "Epoch 72/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0908 - acc: 0.9714 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "Epoch 73/250\n",
      "105/105 [==============================] - 0s 183us/step - loss: 0.0902 - acc: 0.9714 - val_loss: 0.0446 - val_acc: 1.0000\n",
      "Epoch 74/250\n",
      "105/105 [==============================] - 0s 258us/step - loss: 0.0901 - acc: 0.9714 - val_loss: 0.0439 - val_acc: 1.0000\n",
      "Epoch 75/250\n",
      "105/105 [==============================] - 0s 179us/step - loss: 0.0918 - acc: 0.9714 - val_loss: 0.0439 - val_acc: 1.0000\n",
      "Epoch 76/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0904 - acc: 0.9714 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "Epoch 77/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0901 - acc: 0.9810 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "Epoch 78/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0899 - acc: 0.9714 - val_loss: 0.0438 - val_acc: 1.0000\n",
      "Epoch 79/250\n",
      "105/105 [==============================] - 0s 148us/step - loss: 0.0896 - acc: 0.9714 - val_loss: 0.0442 - val_acc: 1.0000\n",
      "Epoch 80/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0918 - acc: 0.9714 - val_loss: 0.0442 - val_acc: 1.0000\n",
      "Epoch 81/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0893 - acc: 0.9714 - val_loss: 0.0438 - val_acc: 1.0000\n",
      "Epoch 82/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0895 - acc: 0.9714 - val_loss: 0.0441 - val_acc: 1.0000\n",
      "Epoch 83/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0952 - acc: 0.9714 - val_loss: 0.0445 - val_acc: 1.0000\n",
      "Epoch 84/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0898 - acc: 0.9714 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 85/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0918 - acc: 0.9714 - val_loss: 0.0448 - val_acc: 1.0000\n",
      "Epoch 86/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0981 - acc: 0.9714 - val_loss: 0.0439 - val_acc: 1.0000\n",
      "Epoch 87/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0939 - acc: 0.9714 - val_loss: 0.0450 - val_acc: 1.0000\n",
      "Epoch 88/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0924 - acc: 0.9714 - val_loss: 0.0470 - val_acc: 1.0000\n",
      "Epoch 89/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0916 - acc: 0.9714 - val_loss: 0.0457 - val_acc: 1.0000\n",
      "Epoch 90/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0916 - acc: 0.9714 - val_loss: 0.0466 - val_acc: 1.0000\n",
      "Epoch 91/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0909 - acc: 0.9714 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 92/250\n",
      "105/105 [==============================] - 0s 360us/step - loss: 0.0915 - acc: 0.9714 - val_loss: 0.0439 - val_acc: 1.0000\n",
      "Epoch 93/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.0990 - acc: 0.9714 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 94/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0937 - acc: 0.9714 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 95/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0893 - acc: 0.9714 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 96/250\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.0479 - acc: 1.000 - 0s 0us/step - loss: 0.0895 - acc: 0.9714 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 97/250\n",
      "105/105 [==============================] - 0s 62us/step - loss: 0.0891 - acc: 0.9714 - val_loss: 0.0435 - val_acc: 1.0000\n",
      "Epoch 98/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0904 - acc: 0.9714 - val_loss: 0.0435 - val_acc: 1.0000\n",
      "Epoch 99/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0915 - acc: 0.9714 - val_loss: 0.0435 - val_acc: 1.0000\n",
      "Epoch 100/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0898 - acc: 0.9714 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 101/250\n",
      "105/105 [==============================] - 0s 211us/step - loss: 0.0890 - acc: 0.9714 - val_loss: 0.0438 - val_acc: 1.0000\n",
      "Epoch 102/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0906 - acc: 0.9714 - val_loss: 0.0442 - val_acc: 1.0000\n",
      "Epoch 103/250\n",
      "105/105 [==============================] - 0s 290us/step - loss: 0.0947 - acc: 0.9714 - val_loss: 0.0460 - val_acc: 1.0000\n",
      "Epoch 104/250\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0904 - acc: 0.9714 - val_loss: 0.0450 - val_acc: 1.0000\n",
      "Epoch 105/250\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0905 - acc: 0.9714 - val_loss: 0.0434 - val_acc: 1.0000\n",
      "Epoch 106/250\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0906 - acc: 0.9714 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "Epoch 107/250\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0932 - acc: 0.9714 - val_loss: 0.0448 - val_acc: 1.0000\n",
      "Epoch 108/250\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0956 - acc: 0.9714 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 109/250\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0901 - acc: 0.9714 - val_loss: 0.0435 - val_acc: 1.0000\n",
      "Epoch 110/250\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0901 - acc: 0.9714 - val_loss: 0.0442 - val_acc: 1.0000\n",
      "Epoch 111/250\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0949 - acc: 0.9714 - val_loss: 0.0439 - val_acc: 1.0000\n",
      "Epoch 112/250\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0890 - acc: 0.9714 - val_loss: 0.0434 - val_acc: 1.0000\n",
      "Epoch 113/250\n",
      "105/105 [==============================] - 0s 134us/step - loss: 0.0900 - acc: 0.9714 - val_loss: 0.0433 - val_acc: 1.0000\n",
      "Epoch 114/250\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0918 - acc: 0.9619 - val_loss: 0.0441 - val_acc: 1.0000\n",
      "Epoch 115/250\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0888 - acc: 0.9714 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 116/250\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0900 - acc: 0.9714 - val_loss: 0.0444 - val_acc: 1.0000\n",
      "Epoch 117/250\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0897 - acc: 0.9714 - val_loss: 0.0444 - val_acc: 1.0000\n",
      "Epoch 118/250\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0897 - acc: 0.9714 - val_loss: 0.0432 - val_acc: 1.0000\n",
      "Epoch 119/250\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0909 - acc: 0.9714 - val_loss: 0.0432 - val_acc: 1.0000\n",
      "Epoch 120/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 105us/step - loss: 0.0911 - acc: 0.9714 - val_loss: 0.0434 - val_acc: 1.0000\n",
      "Epoch 121/250\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0900 - acc: 0.9810 - val_loss: 0.0438 - val_acc: 1.0000\n",
      "Epoch 122/250\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.0890 - acc: 0.9714 - val_loss: 0.0432 - val_acc: 1.0000\n",
      "Epoch 123/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0912 - acc: 0.9810 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 124/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0895 - acc: 0.9714 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 125/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0893 - acc: 0.9714 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 126/250\n",
      "105/105 [==============================] - 0s 38us/step - loss: 0.0926 - acc: 0.9714 - val_loss: 0.0434 - val_acc: 1.0000\n",
      "Epoch 127/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0888 - acc: 0.9714 - val_loss: 0.0433 - val_acc: 1.0000\n",
      "Epoch 128/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0903 - acc: 0.9714 - val_loss: 0.0450 - val_acc: 1.0000\n",
      "Epoch 129/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0891 - acc: 0.9714 - val_loss: 0.0433 - val_acc: 1.0000\n",
      "Epoch 130/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0887 - acc: 0.9714 - val_loss: 0.0433 - val_acc: 1.0000\n",
      "Epoch 131/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0896 - acc: 0.9714 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 132/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0899 - acc: 0.9714 - val_loss: 0.0438 - val_acc: 1.0000\n",
      "Epoch 133/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0887 - acc: 0.9714 - val_loss: 0.0430 - val_acc: 1.0000\n",
      "Epoch 134/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0887 - acc: 0.9714 - val_loss: 0.0435 - val_acc: 1.0000\n",
      "Epoch 135/250\n",
      "105/105 [==============================] - 0s 62us/step - loss: 0.0979 - acc: 0.9714 - val_loss: 0.0461 - val_acc: 1.0000\n",
      "Epoch 136/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0909 - acc: 0.9714 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "Epoch 137/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0894 - acc: 0.9714 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 138/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0891 - acc: 0.9714 - val_loss: 0.0429 - val_acc: 1.0000\n",
      "Epoch 139/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0893 - acc: 0.9714 - val_loss: 0.0429 - val_acc: 1.0000\n",
      "Epoch 140/250\n",
      "105/105 [==============================] - 0s 207us/step - loss: 0.0928 - acc: 0.9714 - val_loss: 0.0433 - val_acc: 1.0000\n",
      "Epoch 141/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0911 - acc: 0.9810 - val_loss: 0.0432 - val_acc: 1.0000\n",
      "Epoch 142/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0894 - acc: 0.9714 - val_loss: 0.0429 - val_acc: 1.0000\n",
      "Epoch 143/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.0946 - acc: 0.9714 - val_loss: 0.0435 - val_acc: 1.0000\n",
      "Epoch 144/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0905 - acc: 0.9810 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 145/250\n",
      "105/105 [==============================] - 0s 207us/step - loss: 0.0888 - acc: 0.9714 - val_loss: 0.0435 - val_acc: 1.0000\n",
      "Epoch 146/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0975 - acc: 0.9714 - val_loss: 0.0428 - val_acc: 1.0000\n",
      "Epoch 147/250\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.0901 - acc: 0.9714 - val_loss: 0.0429 - val_acc: 1.0000\n",
      "Epoch 148/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0894 - acc: 0.9905 - val_loss: 0.0433 - val_acc: 1.0000\n",
      "Epoch 149/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0892 - acc: 0.9714 - val_loss: 0.0427 - val_acc: 1.0000\n",
      "Epoch 150/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0895 - acc: 0.9714 - val_loss: 0.0429 - val_acc: 1.0000\n",
      "Epoch 151/250\n",
      "105/105 [==============================] - 0s 159us/step - loss: 0.0888 - acc: 0.9714 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 152/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0888 - acc: 0.9714 - val_loss: 0.0427 - val_acc: 1.0000\n",
      "Epoch 153/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.1004 - acc: 0.9619 - val_loss: 0.0439 - val_acc: 1.0000\n",
      "Epoch 154/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.0893 - acc: 0.9714 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 155/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0900 - acc: 0.9714 - val_loss: 0.0426 - val_acc: 1.0000\n",
      "Epoch 156/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0890 - acc: 0.9714 - val_loss: 0.0427 - val_acc: 1.0000\n",
      "Epoch 157/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0949 - acc: 0.9714 - val_loss: 0.0454 - val_acc: 1.0000\n",
      "Epoch 158/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0916 - acc: 0.9714 - val_loss: 0.0455 - val_acc: 1.0000\n",
      "Epoch 159/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.0937 - acc: 0.9714 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 160/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0893 - acc: 0.9714 - val_loss: 0.0427 - val_acc: 1.0000\n",
      "Epoch 161/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0890 - acc: 0.9714 - val_loss: 0.0426 - val_acc: 1.0000\n",
      "Epoch 162/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0914 - acc: 0.9714 - val_loss: 0.0426 - val_acc: 1.0000\n",
      "Epoch 163/250\n",
      "105/105 [==============================] - 0s 62us/step - loss: 0.0907 - acc: 0.9714 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Epoch 164/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0885 - acc: 0.9714 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Epoch 165/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0938 - acc: 0.9714 - val_loss: 0.0427 - val_acc: 1.0000\n",
      "Epoch 166/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0884 - acc: 0.9714 - val_loss: 0.0428 - val_acc: 1.0000\n",
      "Epoch 167/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0899 - acc: 0.9810 - val_loss: 0.0447 - val_acc: 1.0000\n",
      "Epoch 168/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0907 - acc: 0.9714 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 169/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0897 - acc: 0.9714 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 170/250\n",
      "105/105 [==============================] - 0s 211us/step - loss: 0.0888 - acc: 0.9714 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 171/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0909 - acc: 0.9714 - val_loss: 0.0434 - val_acc: 1.0000\n",
      "Epoch 172/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0896 - acc: 0.9714 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 173/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0898 - acc: 0.9714 - val_loss: 0.0424 - val_acc: 1.0000\n",
      "Epoch 174/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0894 - acc: 0.9714 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Epoch 175/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0893 - acc: 0.9714 - val_loss: 0.0424 - val_acc: 1.0000\n",
      "Epoch 176/250\n",
      "105/105 [==============================] - 0s 62us/step - loss: 0.0923 - acc: 0.9714 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 177/250\n",
      "105/105 [==============================] - 0s 179us/step - loss: 0.0885 - acc: 0.9714 - val_loss: 0.0429 - val_acc: 1.0000\n",
      "Epoch 178/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0882 - acc: 0.9714 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 179/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0879 - acc: 0.9714 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Epoch 180/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 149us/step - loss: 0.0880 - acc: 0.9714 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 181/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0880 - acc: 0.9714 - val_loss: 0.0428 - val_acc: 1.0000\n",
      "Epoch 182/250\n",
      "105/105 [==============================] - 0s 178us/step - loss: 0.0887 - acc: 0.9714 - val_loss: 0.0434 - val_acc: 1.0000\n",
      "Epoch 183/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0884 - acc: 0.9714 - val_loss: 0.0432 - val_acc: 1.0000\n",
      "Epoch 184/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0888 - acc: 0.9714 - val_loss: 0.0422 - val_acc: 1.0000\n",
      "Epoch 185/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0893 - acc: 0.9714 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Epoch 186/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0888 - acc: 0.9714 - val_loss: 0.0422 - val_acc: 1.0000\n",
      "Epoch 187/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0896 - acc: 0.9714 - val_loss: 0.0430 - val_acc: 1.0000\n",
      "Epoch 188/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0894 - acc: 0.9714 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 189/250\n",
      "105/105 [==============================] - 0s 211us/step - loss: 0.0911 - acc: 0.9714 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 190/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0877 - acc: 0.9714 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 191/250\n",
      "105/105 [==============================] - 0s 68us/step - loss: 0.0932 - acc: 0.9714 - val_loss: 0.0424 - val_acc: 1.0000\n",
      "Epoch 192/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0922 - acc: 0.9810 - val_loss: 0.0426 - val_acc: 1.0000\n",
      "Epoch 193/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0919 - acc: 0.9810 - val_loss: 0.0421 - val_acc: 1.0000\n",
      "Epoch 194/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0886 - acc: 0.9714 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 195/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0915 - acc: 0.9714 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 196/250\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.0516 - acc: 1.000 - 0s 143us/step - loss: 0.0897 - acc: 0.9810 - val_loss: 0.0430 - val_acc: 1.0000\n",
      "Epoch 197/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0911 - acc: 0.9714 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 198/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0911 - acc: 0.9714 - val_loss: 0.0430 - val_acc: 1.0000\n",
      "Epoch 199/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0882 - acc: 0.9714 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Epoch 200/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0949 - acc: 0.9714 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 201/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0896 - acc: 0.9714 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 202/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0882 - acc: 0.9714 - val_loss: 0.0430 - val_acc: 1.0000\n",
      "Epoch 203/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0881 - acc: 0.9714 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "Epoch 204/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0885 - acc: 0.9810 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 205/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0879 - acc: 0.9714 - val_loss: 0.0426 - val_acc: 1.0000\n",
      "Epoch 206/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0909 - acc: 0.9714 - val_loss: 0.0419 - val_acc: 1.0000\n",
      "Epoch 207/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0884 - acc: 0.9714 - val_loss: 0.0424 - val_acc: 1.0000\n",
      "Epoch 208/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0885 - acc: 0.9714 - val_loss: 0.0419 - val_acc: 1.0000\n",
      "Epoch 209/250\n",
      "105/105 [==============================] - 0s 211us/step - loss: 0.0902 - acc: 0.9714 - val_loss: 0.0434 - val_acc: 1.0000\n",
      "Epoch 210/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0881 - acc: 0.9714 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 211/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0893 - acc: 0.9714 - val_loss: 0.0428 - val_acc: 1.0000\n",
      "Epoch 212/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0883 - acc: 0.9714 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 213/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0881 - acc: 0.9714 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 214/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0878 - acc: 0.9714 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 215/250\n",
      "105/105 [==============================] - 0s 62us/step - loss: 0.0878 - acc: 0.9714 - val_loss: 0.0424 - val_acc: 1.0000\n",
      "Epoch 216/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0884 - acc: 0.9714 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "Epoch 217/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0888 - acc: 0.9714 - val_loss: 0.0430 - val_acc: 1.0000\n",
      "Epoch 218/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0877 - acc: 0.9714 - val_loss: 0.0424 - val_acc: 1.0000\n",
      "Epoch 219/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.0885 - acc: 0.9714 - val_loss: 0.0419 - val_acc: 1.0000\n",
      "Epoch 220/250\n",
      "105/105 [==============================] - 0s 211us/step - loss: 0.0896 - acc: 0.9714 - val_loss: 0.0422 - val_acc: 1.0000\n",
      "Epoch 221/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0912 - acc: 0.9714 - val_loss: 0.0421 - val_acc: 1.0000\n",
      "Epoch 222/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0888 - acc: 0.9714 - val_loss: 0.0422 - val_acc: 1.0000\n",
      "Epoch 223/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0926 - acc: 0.9714 - val_loss: 0.0417 - val_acc: 1.0000\n",
      "Epoch 224/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.0905 - acc: 0.9714 - val_loss: 0.0422 - val_acc: 1.0000\n",
      "Epoch 225/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0895 - acc: 0.9714 - val_loss: 0.0417 - val_acc: 1.0000\n",
      "Epoch 226/250\n",
      "105/105 [==============================] - 0s 62us/step - loss: 0.0887 - acc: 0.9714 - val_loss: 0.0421 - val_acc: 1.0000\n",
      "Epoch 227/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0873 - acc: 0.9714 - val_loss: 0.0419 - val_acc: 1.0000\n",
      "Epoch 228/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0877 - acc: 0.9714 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Epoch 229/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0890 - acc: 0.9714 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 230/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0877 - acc: 0.9714 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 231/250\n",
      "105/105 [==============================] - 0s 211us/step - loss: 0.0881 - acc: 0.9714 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 232/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0875 - acc: 0.9714 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 233/250\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.0876 - acc: 0.9714 - val_loss: 0.0415 - val_acc: 1.0000\n",
      "Epoch 234/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0875 - acc: 0.9714 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 235/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0908 - acc: 0.9714 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 236/250\n",
      "105/105 [==============================] - 0s 62us/step - loss: 0.0906 - acc: 0.9714 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "Epoch 237/250\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0895 - acc: 0.9714 - val_loss: 0.0415 - val_acc: 1.0000\n",
      "Epoch 238/250\n",
      "105/105 [==============================] - 0s 156us/step - loss: 0.0908 - acc: 0.9714 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 239/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0886 - acc: 0.9714 - val_loss: 0.0417 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0902 - acc: 0.9810 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 241/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0872 - acc: 0.9714 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 242/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0879 - acc: 0.9714 - val_loss: 0.0427 - val_acc: 1.0000\n",
      "Epoch 243/250\n",
      "105/105 [==============================] - 0s 192us/step - loss: 0.0884 - acc: 0.9714 - val_loss: 0.0414 - val_acc: 1.0000\n",
      "Epoch 244/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0872 - acc: 0.9714 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 245/250\n",
      "105/105 [==============================] - 0s 0us/step - loss: 0.0912 - acc: 0.9714 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 246/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0881 - acc: 0.9714 - val_loss: 0.0415 - val_acc: 1.0000\n",
      "Epoch 247/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0882 - acc: 0.9714 - val_loss: 0.0417 - val_acc: 1.0000\n",
      "Epoch 248/250\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0878 - acc: 0.9714 - val_loss: 0.0427 - val_acc: 1.0000\n",
      "Epoch 249/250\n",
      "105/105 [==============================] - 0s 150us/step - loss: 0.0884 - acc: 0.9714 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 250/250\n",
      "105/105 [==============================] - 0s 197us/step - loss: 0.0964 - acc: 0.9714 - val_loss: 0.0413 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "a = accuracy_score(pred,test)\n",
    "print('Accuracy is:', a*100)\n",
    "\n",
    "history = model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=250, batch_size=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sensor values: \n",
      "4.5\n",
      "1.8\n",
      "3.5\n",
      "2\n",
      "Non-roundness is the defect detected!\n"
     ]
    }
   ],
   "source": [
    "print('Enter the sensor values: ')\n",
    "s1=float(input())\n",
    "s2=float(input())\n",
    "s3=float(input())\n",
    "s4=float(input())\n",
    "res=np.argmax(model.predict([[[s1,s2,s3,s4]]]))\n",
    "if res==0:\n",
    "    print('Flatspot is the defect detected!')\n",
    "elif res==2:\n",
    "    print('Shelling is the defect detected!')\n",
    "elif res==1:\n",
    "    print('Non-roundness is the defect detected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the accuracy and plotting the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 97.77777777777777\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy is:', a*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
